{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d895109",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b3bce",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "<br>\n",
    "\n",
    "* ### Preface\n",
    "<br>\n",
    "* ### Section 1: Decision Tree Classifier\n",
    "    • Weather dataset\n",
    "    <br>\n",
    "    •  Iris Flower dataset\n",
    "    <br>\n",
    "    •  Titanic Dataset\n",
    "<br>\n",
    "<br>\n",
    "* ### Section 2: Decision Tree Regressor\n",
    "    •  House Price Prediction dataset\n",
    "<br>\n",
    "<br>\n",
    "* ### Section 3: Model Performance Comparison\n",
    "    •  Sklearn Classifier on Iris Flower dataset\n",
    "<br>\n",
    "<br>\n",
    "* ### Section 4: Hypothesis Testing on Titanic dataset\n",
    "   \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* ### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c034be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4898d",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba3f86",
   "metadata": {},
   "source": [
    "This notebook implements the following:\n",
    "\n",
    "<br>\n",
    "\n",
    "* Decision Tree Classifier (ID3) capable of making multi-class classifications using continous and categorical features. The model is tested on the Weather dataset as well as on the Iris and Titanic datasets to demonstrate it's ability of making predictions on datasets that contain both types of features.\n",
    "<br>\n",
    "\n",
    "* Decision Tree Regressor capable of making continous values predictions using both numerical and categorical features. Grid-search method is implemented to optimise hyper-parameters on the House price prediction dataset.\n",
    "<br>\n",
    "\n",
    "* Sklearn DecisionTreeClassifier is implemented to contextualise performance of the implementation in Section 1. \n",
    "<br>\n",
    "\n",
    "* Hypothesis testing on Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe4450",
   "metadata": {},
   "source": [
    "## Section 1: Decision Tree Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf4da566",
   "metadata": {},
   "source": [
    "The Decision Tree Classifier is implemented using object oriented programming. The main class that instantiates the model is `DecisionTreeClassifier`. <br>\n",
    "Upon instantiation the `train_test_split` method can be called to get a two way split of the entire dataset: one for training and one for testing. The paremeters to feed in are: \n",
    "\n",
    "<br>\n",
    "\n",
    "* `df`: entire dataset\n",
    "* `test_size`: percentage of dataset to split [0-1].\n",
    "\n",
    "<br>\n",
    "\n",
    "Once datasets have been split, the `fit` method is used to fit the tree on the training dataset. This method takes in parameters:\n",
    "\n",
    "<br>\n",
    "\n",
    "* `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "* `min_samples_split`: The minimum number of samples required to split an internal node\n",
    "\n",
    "<br>\n",
    "\n",
    "The tree is built recursively where at each iteration, method `features_splits` is called to determine the split points for categorical and continuous features. `best_split` is used calculate the best feature to split based on its value which is derived using the following entropy formula in the `weighted_entropy`method: \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "Entropy = \\sum_{i=1}^c -p_i\\log_2{p_i}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "The data is then split into left and right nodes/leafs using `data_split` mothod.\n",
    "<br>\n",
    "\n",
    "To verify the model's accuracy, method `accuracy_score` can be called on the instantiated model. Predictions are made by the `predict` method which runs through the nodes and compares current values with fitted ones. To further inspect correctness of predictions, method `get_precision_recall` returns the percentage values for Precision (i.e. the number of positive class predictions that belong to the positive class):\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "and Recall (i.e. the number of positive class predictions made out of all positive examples in the dataset): \n",
    "\n",
    "\\begin{equation*}\n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "\\end{equation*}\n",
    "\n",
    "Note this method supports binary labels datasets.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The `DecisionTreeClassifier` class has a method called `tree_print` that prints out the fitted tree. This method uses multiple attributes to align the node and leaf boxes to form a tree structure. This is printed by looping through a dictionary containing the tree levels as keys and nodes/leafs as lists of string objects. Worth noting that in the interest of outputting a user readible tree structure, for tress with more than 3 levels of depth a nested lists of dictionaries structure is used to print out the tree. Please reference the dataset sections for more detail. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06baaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.cols = None\n",
    "        self.classify_feature_type = None\n",
    "        self.counter = 0\n",
    "        self.precision_recall_df = None\n",
    "    \n",
    "        # Tree print attibutes\n",
    "        self.full_tree = {}\n",
    "        self.current_sate = ''\n",
    "        self.root_point = int(40 * 1.5)\n",
    "        self.left_root = int(25 * 1.5)\n",
    "        self.right_root = int(10 * 1.5)\n",
    "        self.left_for_right_node = self.root_point\n",
    "        self.right_for_right_node = int(10 * 1.5)\n",
    "        self.left_for_left_node = int(30 * 1.5)\n",
    "        self.right_for_left_node = int(10 * 1.5)\n",
    "        self.drif = 4 \n",
    "        self.box_len = 25\n",
    "        self.level_space = 2\n",
    "        self.left_state = 0\n",
    "        self.print_holder = {}\n",
    "        self.print_leaf = {}\n",
    "        self.print_tree_higher_depth = None\n",
    "\n",
    "\n",
    "    \n",
    "    def get_purity(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to check purity of labels Y.\n",
    "        :param data: dataset.\n",
    "        :return: Boolean true or false.\n",
    "        \"\"\"\n",
    "\n",
    "        labels = data[:, -1]\n",
    "        unique_labels = np.unique(labels)\n",
    "\n",
    "        if len(unique_labels) == 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def classify_data(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        :input data: dataset.\n",
    "        :return: most frequent label.\n",
    "        \"\"\"\n",
    "\n",
    "        labels = data[:, -1]\n",
    "            \n",
    "        # Classification\n",
    "        unique_classes, unique_classes_counts = np.unique(labels, return_counts=True)\n",
    "        index = unique_classes_counts.argmax()\n",
    "        classification = unique_classes[index]\n",
    "\n",
    "        return classification\n",
    "\n",
    "    def features_splits(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Determine split points for categorical and continuous features.\n",
    "        :input data: dataset.\n",
    "        :return: dictionary of split points by feature.\n",
    "        \"\"\"\n",
    "\n",
    "        splits = {}\n",
    "        n_rows, n_cols = data.shape\n",
    "        for idx_col in range(n_cols - 1): # ignore last column as its label\n",
    "            values = data[:, idx_col]\n",
    "            unique_entries = np.unique(values)\n",
    "\n",
    "            # Check if feature is continous or categorical\n",
    "            feature_type = self.classify_feature_type[idx_col]\n",
    "\n",
    "            # Continuous feature\n",
    "            if feature_type == \"continuous\":\n",
    "                splits[idx_col] = []\n",
    "                for idx in range(len(unique_entries)):\n",
    "                    if idx != 0:\n",
    "                        current_value = unique_entries[idx]\n",
    "                        previous_value = unique_entries[idx - 1]\n",
    "                        potential_split = (current_value + previous_value) / 2\n",
    "                        splits[idx_col].append(potential_split)\n",
    "\n",
    "            # Categorical feature\n",
    "            elif len(unique_entries) > 1:\n",
    "                splits[idx_col] = unique_entries\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def data_split(self, data, col_split, val_split):\n",
    "\n",
    "        \"\"\"\n",
    "        Split continous and categorical data into left and right nodes/leafs. \n",
    "        :input data: dataset.\n",
    "        :input col_split: split dataset columns wise.\n",
    "        :input val_split: split data left and right based on value.\n",
    "        :return: left and right datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        split_column_values = data[:, col_split]\n",
    "        type_of_feature = self.classify_feature_type[col_split]\n",
    "\n",
    "        # Continuous feature\n",
    "        if type_of_feature == \"continuous\":\n",
    "            left_df = data[split_column_values <= val_split]\n",
    "            right_df = data[split_column_values >  val_split]\n",
    "\n",
    "        # Categorical feature \n",
    "        else:\n",
    "            left_df = data[split_column_values == val_split]\n",
    "            right_df = data[split_column_values != val_split]\n",
    "\n",
    "        return left_df, right_df\n",
    "\n",
    "\n",
    "    def entropy(self, data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to calculate entropy.\n",
    "        :param data: dataset.\n",
    "        :return: calculated entropy.\n",
    "        \"\"\"\n",
    "\n",
    "        labels = data[:, -1]\n",
    "        vals, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "        percent_count = counts / counts.sum()\n",
    "        entropy = sum(percent_count * -np.log2(percent_count))\n",
    "\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def weighted_entropy(self, left_df, right_df):\n",
    "\n",
    "        \"\"\"\n",
    "        Given left and right df calculate full entropy value.\n",
    "        :input left_df: 'positive' value dataset. \n",
    "        :input right_df: 'negative' value dataset.\n",
    "        :return: weighted entropy\n",
    "        \"\"\"\n",
    "\n",
    "        tot = len(left_df) + len(right_df)\n",
    "        perc_left_df = len(left_df) / tot\n",
    "        perc_right_df = len(right_df) / tot\n",
    "\n",
    "        full_entropy = (perc_left_df * self.entropy(left_df) \n",
    "                        + perc_right_df * self.entropy(right_df))\n",
    "\n",
    "        return full_entropy\n",
    "\n",
    "    \n",
    "    def best_split(self, data, splits):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to calculate the best feature, its split point and corresponding value. \n",
    "        :input data: dataset. \n",
    "        :input splits: dictionary of features and corresponding split points.\n",
    "        :return: best feature and its value. \n",
    "        \"\"\"\n",
    "\n",
    "        entropy = np.inf\n",
    "        for column in splits:\n",
    "            for value in splits[column]:\n",
    "                left_df, right_df = self.data_split(data, col_split=column, val_split=value)\n",
    "                self.current_entropy = self.weighted_entropy(left_df, right_df)\n",
    "                \n",
    "                if self.current_entropy <= entropy:\n",
    "                    entropy = self.current_entropy\n",
    "                    self.best_split_column = column\n",
    "                    self.best_split_value = value\n",
    "\n",
    "        return self.best_split_column, self.best_split_value, self.current_entropy\n",
    "\n",
    "\n",
    "    def get_feature_type(self,df):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to differentiate features based of data type (categorical, continous)\n",
    "        :input df: dataframe.\n",
    "        :return: list of strings for continous and categorical features.\n",
    "        \"\"\"\n",
    "\n",
    "        features_type = []\n",
    "        for feature in df.columns:\n",
    "            # Don't include label\n",
    "            if feature != \"Y\":\n",
    "                unique_values = df[feature].unique()\n",
    "                test_value = unique_values[0]\n",
    "                # Classify features \n",
    "                if (isinstance(test_value, str)):\n",
    "                    features_type.append(\"categorical\")\n",
    "                else:\n",
    "                    features_type.append(\"continuous\")\n",
    "\n",
    "        return features_type\n",
    "\n",
    "    \n",
    "    def fit(self, df, max_depth, min_samples_split):\n",
    "    \n",
    "        \"\"\"\n",
    "        Function that builds the decision tree recursively.\n",
    "        :input df: dataframe.\n",
    "        :input max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until \n",
    "                          all leaves contain less than min_samples_split samples.\n",
    "        :input min_samples_split: The minimum number of samples required to split an internal node\n",
    "        :return: whole decision tree structure.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Data preparations\n",
    "        if self.counter < 1:\n",
    "            self.cols = df.columns\n",
    "            self.classify_feature_type = self.get_feature_type(df)\n",
    "            data = df.values\n",
    "        else:\n",
    "            data = df    \n",
    "        \n",
    "        # Base case: either pure or breaching thresholds\n",
    "        if (self.counter == max_depth) | (self.get_purity(data)) | (len(data) < min_samples_split) :\n",
    "            classification = self.classify_data(data)\n",
    "\n",
    "            return classification\n",
    "\n",
    "        # Recursion\n",
    "        else:    \n",
    "            self.counter += 1\n",
    "\n",
    "            # Find split points\n",
    "            get_splits = self.features_splits(data)\n",
    "            col_split, self.val_split, self.curr_entropy = self.best_split(data, get_splits)\n",
    "            left_df, right_df = self.data_split(data, col_split, self.val_split)\n",
    "            \n",
    "            # Continous feature\n",
    "            self.feature_name = self.cols[col_split]\n",
    "            feature_type = self.classify_feature_type[col_split]            \n",
    "            \n",
    "            if feature_type == \"continuous\":\n",
    "                tree = \"{} <= {}\".format(self.feature_name, self.val_split)\n",
    "                \n",
    "                # Store data to print tree\n",
    "                self.print_holder[self.counter] = [self.feature_name, \n",
    "                                                   '<=',\n",
    "                                                   self.val_split, \n",
    "                                                   round(self.curr_entropy,2)\n",
    "                                                  ]\n",
    "            \n",
    "            # Categorical feature\n",
    "            else:\n",
    "                tree = \"{} = {}\".format(self.feature_name, self.val_split)\n",
    "                \n",
    "                # Store data to print tree\n",
    "                self.print_holder[self.counter] = [self.feature_name,\n",
    "                                                   '=',\n",
    "                                                   self.val_split, \n",
    "                                                   round(self.curr_entropy,2)\n",
    "                                                  ]\n",
    "\n",
    "            # instantiate sub-tree\n",
    "            sub_tree = {tree: []}\n",
    "                        \n",
    "            # Expand recursion\n",
    "            left_yes = self.fit(left_df, max_depth, min_samples_split)\n",
    "            right_no = self.fit(right_df, max_depth, min_samples_split)     \n",
    "            \n",
    "            if left_yes == right_no:\n",
    "                sub_tree = left_yes\n",
    "            else:\n",
    "                sub_tree[tree].append(left_yes)\n",
    "                sub_tree[tree].append(right_no)\n",
    "        \n",
    "        # Store tree\n",
    "        self.print_tree_higher_depth = sub_tree\n",
    "            \n",
    "        return sub_tree\n",
    "\n",
    "    \n",
    "    def predict(self,data, tree):\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Function to predict new data using the fitted tree.\n",
    "        :input data: data to predict.\n",
    "        :input tree: fitted tree. \n",
    "        :return: predictions\n",
    "        \"\"\"\n",
    "\n",
    "        question = list(tree.keys())[0]\n",
    "        feature_name, symbol, value = question.split(\" \")\n",
    "\n",
    "        # Continuous festure split question\n",
    "        if symbol == \"<=\":  \n",
    "            if data[feature_name] <= float(value):\n",
    "                answer = tree[question][0]\n",
    "            else:\n",
    "                answer = tree[question][1]\n",
    "\n",
    "        # Categorical feature split question \n",
    "        else:\n",
    "            if str(data[feature_name]) == value:\n",
    "                answer = tree[question][0]\n",
    "            else:\n",
    "                answer = tree[question][1]\n",
    "\n",
    "        # Base case\n",
    "        if not isinstance(answer, dict):\n",
    "            return answer\n",
    "\n",
    "        # Recursion\n",
    "        else:\n",
    "            residual_tree = answer\n",
    "            return self.predict(data, residual_tree)\n",
    "\n",
    "\n",
    "    def accuracy_score(self, df, tree):\n",
    "\n",
    "        \"\"\"\n",
    "        Given predictions calculate accuracy.\n",
    "        :input df: dataframe. \n",
    "        :input tree: fitted tree. \n",
    "        :return: Accuracy figures in text format\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store predictions\n",
    "        df[\"Y_predict\"] = df.apply(self.predict, axis=1, args=(tree,))\n",
    "\n",
    "        # Check if prediction match \n",
    "        df[\"Is_prediction_accurate\"] = df[\"Y_predict\"] == df[\"Y\"]\n",
    "        \n",
    "        self.precision_recall_df = df.copy()\n",
    "        \n",
    "        global a \n",
    "        \n",
    "        a = df.copy()\n",
    "\n",
    "        # Get mean prediction accuracy \n",
    "        mean_accuracy = df[\"Is_prediction_accurate\"].mean()\n",
    "\n",
    "        accuracy_string = f'The Decision Tree Classifier accuracy is: {round(mean_accuracy,2)*100} %'\n",
    "\n",
    "        return accuracy_string\n",
    "    \n",
    "    \n",
    "    def calculate_precision_recall(self):\n",
    "        \n",
    "        # 0 - 0 true negative\n",
    "        self.precision_recall_df['True Negative'] = self.precision_recall_df.apply(lambda x: 1 if x['Y'] == 0 and x['Y_predict'] == 0 else 0, axis=1)\n",
    "\n",
    "        # 0 - 1 false positive\n",
    "        self.precision_recall_df['False Positive'] = self.precision_recall_df.apply(lambda x: 1 if x['Y'] == 0 and x['Y_predict'] == 1 else 0, axis=1)\n",
    "\n",
    "        # 1 - 1 true positive \n",
    "        self.precision_recall_df['True Positive'] = self.precision_recall_df.apply(lambda x: 1 if x['Y'] == 1 and x['Y_predict'] == 1 else 0, axis=1)\n",
    "\n",
    "        # 1 - 0 false negative\n",
    "        self.precision_recall_df['False Negative'] = self.precision_recall_df.apply(lambda x: 1 if x['Y'] == 1 and x['Y_predict'] == 0 else 0, axis=1)\n",
    "\n",
    "        # Precision = TP/(TP+FP)\n",
    "        precision = round((self.precision_recall_df['True Positive'].sum()/(self.precision_recall_df['True Positive'].sum()+\\\n",
    "                                                                self.precision_recall_df['False Positive'].sum()))*100,2)\n",
    "        # Recall TP/(TP+FN)\n",
    "        recall = round((self.precision_recall_df['True Positive'].sum()/(self.precision_recall_df['True Positive'].sum()+\\\n",
    "                                                                self.precision_recall_df['False Negative'].sum()))*100,2)\n",
    "\n",
    "        return f\"The Decision Tree Classifier has a Precision of {precision}% and Recall of {recall}%\"\n",
    "\n",
    "    \n",
    "    def train_test_split(self, df, test_size):\n",
    "\n",
    "        \"\"\"\n",
    "        Split data into train and test.\n",
    "        :param df: entire dataset.\n",
    "        :param test_size: if < 0 -> % of dataset allocated to test else number of rows.\n",
    "        return: train and test datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if test size if float\n",
    "        if isinstance(test_size, float):\n",
    "            test_size = round(test_size * len(df))\n",
    "\n",
    "        idx = df.index.tolist()\n",
    "\n",
    "        # Get test data\n",
    "        test_rows = random.sample(population=idx, k=test_size)\n",
    "\n",
    "        test_df = df.loc[test_rows]\n",
    "        train_df = df.drop(test_rows)\n",
    "\n",
    "        return train_df, test_df\n",
    "\n",
    "    \n",
    "    def get_dict_tree(self, tree, iter=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Get the fitted tree and add structure to the data ready to facilitate the print out.\n",
    "        :input tree: fitted tree\n",
    "        :input iter: parameter to count the tree depth\n",
    "        :return: dictionary to be consumed by tree_print() function.  \n",
    "        \"\"\"\n",
    "\n",
    "        if iter is None:\n",
    "            iter = 0\n",
    "\n",
    "        for k, v in tree.items():\n",
    "\n",
    "            if iter is None:\n",
    "                iter = 0\n",
    "                print('ROOT')\n",
    "\n",
    "            # Is iter key already present?\n",
    "            if iter not in self.full_tree.keys():\n",
    "                self.full_tree[iter] = [k]\n",
    "            else: \n",
    "                self.full_tree[iter].append(k)\n",
    "\n",
    "            iter+=1\n",
    "\n",
    "            # Root\n",
    "            if type(v) == dict:\n",
    "                get_dict_tree(v, iter)\n",
    "\n",
    "            # Left node\n",
    "            elif type(v) == list:\n",
    "                if type(v[0]) == dict:\n",
    "                    self.get_dict_tree(v[0],iter)\n",
    "\n",
    "                # Left Leaf\n",
    "                elif (type(v[0]) == str) or (type(v[0]) == int):\n",
    "                    if iter not in self.full_tree.keys():\n",
    "                        self.full_tree[iter] = ['left leaf',v[0]]\n",
    "                    else: \n",
    "                        self.full_tree[iter].extend(['left leaf',v[0]])\n",
    "\n",
    "                # Right node\n",
    "                if type(v[1]) == dict:\n",
    "                    self.get_dict_tree(v[1],iter)\n",
    "\n",
    "                # Right leaf    \n",
    "                elif (type(v[1]) == str) or (type(v[1]) == int):\n",
    "                    if iter not in self.full_tree.keys():\n",
    "                        self.full_tree[iter] = ['right leaf',v[1]]\n",
    "                    else:\n",
    "                        self.full_tree[iter].extend(['right leaf',v[1]])\n",
    "\n",
    "        return self.full_tree\n",
    "\n",
    "    \n",
    "    def tree_print(self, full_tree):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to print the decision tree. \n",
    "        :input full_tree: Dictionary where keys corresponds to each tree level and values to left and right leaf or node.\n",
    "        :return: print out of the entire decision tree. \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.counter < 4: \n",
    "        \n",
    "            # Loop through each row of the tree\n",
    "            for k, v in full_tree.items():\n",
    "\n",
    "                # Root \n",
    "                if k == 0:\n",
    "                    print(' '*self.root_point,'='*(len(v[0])+4))\n",
    "                    print(' '*self.root_point,' '*((len(v[0])-4)//2),'ROOT',' '*5)\n",
    "                    print(' '*self.root_point, v[0])\n",
    "                    print(' '*self.root_point, f\"Entropy: {self.print_holder[k+1][3]}\")\n",
    "                    print(' '*self.root_point,'='*(len(v[0])+4),'\\n'*self.level_space)\n",
    "                    self.current_sate = 'root'\n",
    "\n",
    "                else:\n",
    "\n",
    "                    for idx, i in enumerate(v):\n",
    "\n",
    "                        # Dont include root\n",
    "                        if idx >= 1: \n",
    "\n",
    "                            # right node, left node\n",
    "                            if len(v) == 2:\n",
    "                                if self.current_sate == 'root':\n",
    "\n",
    "                                    print(' '*self.left_root,'='*self.box_len, ' '*self.right_root, '='*self.box_len)\n",
    "                                    print(' '*self.left_root,f'LEFT NODE Level {k}',' '*(self.box_len - len('Left node Level k')),\\\n",
    "                                          ' '*self.right_root,f'RIGHT NODE Level {k}',' '*(self.box_len - len('Right node Level k')))\n",
    "                                    print(' '*self.left_root,v[0],' '*(self.box_len - len(v[0])),\\\n",
    "                                          ' '*self.right_root,v[1],' '*(self.box_len - len(v[1])))\n",
    "\n",
    "                                    print(' '*self.left_root,f\"Entropy: {self.print_holder[k+1][3]}\",' '*(self.box_len-len(f\"Entropy: {self.print_holder[k+1][3]}\")),\\\n",
    "                                          ' '*self.right_root,f\"Entropy: {self.print_holder[k+1][3]}\",' '*(self.box_len - len(v[1])))                                \n",
    "\n",
    "                                    print(' '*self.left_root,'='*self.box_len, ' '*self.right_root, '='*self.box_len,'\\n'*3)\n",
    "\n",
    "                                    self.current_sate = 'right left node'\n",
    "                                    break\n",
    "\n",
    "                                elif self.current_sate == 'right node':\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),'='*self.box_len, ' '*self.right_for_right_node, '='*self.box_len)\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),f'LEFT NODE Level {k}',' '*(self.box_len - len('Left node Level k')),\\\n",
    "                                          ' '*self.right_for_right_node,f'RIGHT NODE Level {k}',' '*(self.box_len - len('Right node Level k')))\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1**self.drif))),v[0],' '*(self.box_len - len(v[0])),\\\n",
    "                                          ' '*self.right_for_right_node,v[1],' '*(self.box_len - len(v[1])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1**self.drif))),self.right_root,f\"Entropy: {self.print_holder[k+1][3]}\",' '*(self.box_len - len(v[0])),\\\n",
    "                                          ' '*self.right_for_right_node,f\"Entropy: {self.print_holder[k+1][3]}\",' '*(self.box_len - len(v[1])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),'='*self.box_len, ' '*self.right_for_right_node, '='*self.box_len,'\\n'*3)\n",
    "\n",
    "                                    self.current_sate = 'right left node'\n",
    "                                    break\n",
    "\n",
    "                                elif self.current_sate == 'left node':\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),'='*self.box_len, ' '*self.right_for_left_node, '='*self.box_len)\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),f'LEFT NODE Level {k}',' '*(self.box_len - len('Left node Level k')),\\\n",
    "                                          ' '*self.right_for_left_node,f'RIGHT NODE Level {k}',' '*(self.box_len - len('Right node Level k')))\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),v[0],' '*(self.box_len - len(v[0])),\\\n",
    "                                          ' '*self.right_for_left_node,v[1],' '*(self.box_len - len(v[1])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),f\"Entropy: {self.print_holder[k+1][3]}\",' '*(self.box_len - len(v[0])),\\\n",
    "                                          ' '*self.right_for_left_node,f\"Entropy: {self.print_holder[k+1][3]}\",' '*(self.box_len - len(v[1])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),'='*self.box_len, ' '*self.right_for_left_node, '='*self.box_len,'\\n'*3) \n",
    "\n",
    "                                    self.current_sate = 'right left node'\n",
    "                                    break\n",
    "\n",
    "\n",
    "                            # left leaf, right node\n",
    "                            if 'left leaf' == v[0] and len(v) <4:\n",
    "                                if self.current_sate == 'root':\n",
    "\n",
    "                                    print(' '*self.left_root,'='*self.box_len, ' '*self.right_root, '='*self.box_len)\n",
    "                                    print(' '*self.left_root,f'LEFT leaf Level {k}',' '*(self.box_len - len('Left leaf Level k')),\\\n",
    "                                          ' '*self.right_root,f'RIGHT NODE Level {k}',' '*(self.box_len - len('Right node Level k')))\n",
    "\n",
    "                                    # Numerical                                 \n",
    "                                    if type(v[1]) == int: # left leaf\n",
    "                                        if v[1] == 1:\n",
    "                                            v[1] = 'Yes'\n",
    "                                        else:\n",
    "                                            v[1] = 'No'\n",
    "\n",
    "                                    print(' '*self.left_root,v[1],' '*(self.box_len - len(v[1])),\\\n",
    "                                          ' '*self.right_root,v[2],' '*(self.box_len - len(v[2])))\n",
    "\n",
    "                                    print(' '*self.left_root,'='*(self.box_len),\\\n",
    "                                          ' '*self.right_root,f\" Entropy: {self.print_holder[k+1][3]}\")\n",
    "\n",
    "                                    print(' '*self.left_root,' '*self.box_len, ' '*self.right_root, '='*self.box_len,'\\n'*self.level_space)  \n",
    "\n",
    "\n",
    "                                    self.current_sate = 'right node'\n",
    "                                    break\n",
    "\n",
    "                                elif self.current_sate == 'right node':\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),'='*self.box_len, ' '*self.right_for_right_node, '='*self.box_len)\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),f'LEFT leaf Level {k}',' '*(self.box_len - len('Left leaf Level k')),\\\n",
    "                                          ' '*self.right_for_right_node,f'RIGHT NODE Level {k}',' '*(self.box_len - len('Right node Level k')))\n",
    "                                    # Numerical\n",
    "                                    if type(v[1]) == int: # left leaf\n",
    "                                        if v[1] == 1:\n",
    "                                            v[1] = 'Yes'\n",
    "                                        else:\n",
    "                                            v[1] = 'No'\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),v[1],' '*(self.box_len - len(v[1])),\\\n",
    "                                          ' '*self.right_for_right_node,v[2],' '*(self.box_len - len(v[2])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),'='*self.box_len, ' '*self.right_for_right_node,\\\n",
    "                                          f\" Entropy: {self.print_holder[k+1][3]}\")\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),' '*self.box_len, ' '*self.right_for_right_node, '='*self.box_len,'\\n'*3)\n",
    "\n",
    "                                    self.current_sate = 'right node'\n",
    "                                    break\n",
    "\n",
    "                                elif self.current_sate == 'left node':\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),'='*self.box_len, ' '*self.right_for_left_node, '='*self.box_len)\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),f'LEFT leaf Level {k}',' '*(self.box_len - len('Left leaf Level k')),\\\n",
    "                                          ' '*self.right_for_left_node,f'RIGHT NODE Level {k}',' '*(self.box_len - len('Right node Level k')))\n",
    "                                    # Numerical\n",
    "                                    if type(v[1]) == int: # left leaf\n",
    "                                        if v[1] == 1:\n",
    "                                            v[1] = 'Yes'\n",
    "                                        else:\n",
    "                                            v[1] = 'No'\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),v[1],' '*(self.box_len - len(v[1])),\\\n",
    "                                          ' '*self.right_for_left_node,v[2],' '*(self.box_len - len(v[2])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),'='*self.box_len,\\\n",
    "                                          ' '*self.right_for_left_node), f\" Entropy: {self.print_holder[k+1][3]}\"\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((k-1)**self.drif))),' '*self.box_len, ' '*self.right_for_left_node, '='*self.box_len,'\\n'*3) \n",
    "\n",
    "                                    self.current_sate = 'right node'\n",
    "                                    break\n",
    "\n",
    "                            # left node, right leaf\n",
    "                            if 'right leaf' == v[1] and len(v) <= 4:\n",
    "                                if self.current_sate == 'root':\n",
    "\n",
    "                                    print(' '*self.left_root,'='*self.box_len, ' '*self.right_root, '='*self.box_len)\n",
    "                                    print(' '*self.left_root,f'LEFT NODE Level {k}',' '*(self.box_len - len('Left node Level k')),\\\n",
    "                                          ' '*self.right_root,f'RIGHT leaf Level {k}',' '*(self.box_len - len('Right leaf Level k')))\n",
    "\n",
    "                                    # Numerical  \n",
    "                                    if type(v[2]) == int: # right leaf\n",
    "                                        if v[2] == 1:\n",
    "                                            v[2] = 'Yes'\n",
    "                                        else:\n",
    "                                            v[2] = 'No'\n",
    "\n",
    "                                    print(' '*self.left_root,v[0],' '*(self.box_len - len(v[0])),\\\n",
    "                                          ' '*self.right_root,v[2],' '*(self.box_len - len(v[2])))\n",
    "\n",
    "                                    print(' '*self.left_root,f\"Entropy: {self.print_holder[k+1][3]}\",' '*(self.box_len-len(f\"Entropy:  {self.print_holder[k+1][3]}\")),\\\n",
    "                                          ' '*(self.right_root),'='*self.box_len)\n",
    "\n",
    "                                    print(' '*self.left_root,'='*self.box_len,'\\n'*3)\n",
    "\n",
    "                                    self.current_sate = 'left node'\n",
    "                                    break\n",
    "\n",
    "                                elif self.current_sate == 'right node':\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),'='*self.box_len, ' '*self.right_for_right_node, '='*self.box_len)\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),f'LEFT NODE Level {k}',' '*(self.box_len - len('Left node Level k')),\\\n",
    "                                          ' '*self.right_for_right_node,f'RIGHT leaf Level {k}',' '*(self.box_len - len('Right leaf Level k')))\n",
    "                                    # Numerical\n",
    "                                    if type(v[2]) == int: # right leaf\n",
    "                                        if v[2] == 1:\n",
    "                                            v[2] = 'Yes'\n",
    "                                        else:\n",
    "                                            v[2] = 'No'\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),v[0],' '*(self.box_len - len(v[0])),\\\n",
    "                                          ' '*self.right_for_right_node,v[2],' '*(self.box_len - len(v[2])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),f\"Entropy: {self.print_holder[k+1][3]}\",\\\n",
    "                                          ' '*((self.right_for_right_node)+(len(v[0]))-7),'='*self.box_len)\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),'='*self.box_len,'\\n'*3)\n",
    "\n",
    "                                    self.current_sate = 'left node'\n",
    "                                    break\n",
    "\n",
    "                                elif self.current_sate == 'left node':\n",
    "\n",
    "                                    # Adjust k for left node\n",
    "                                    adjK = k + 1.3\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((adjK-1)**self.drif))),'='*self.box_len, ' '*self.right_for_left_node, '='*self.box_len)\n",
    "                                    print(' '*int((self.left_for_left_node-((adjK-1)**self.drif))),f'LEFT NODE Level {k}',' '*(self.box_len - len('Left node Level k')),\\\n",
    "                                          ' '*self.right_for_left_node,f'RIGHT leaf Level {k}',' '*(self.box_len - len('Right leaf Level k')))\n",
    "                                    # Numerical\n",
    "                                    if type(v[2]) == int: # right leaf\n",
    "                                        if v[2] == 1:\n",
    "                                            v[2] = 'Yes'\n",
    "                                        else:\n",
    "                                            v[2] = 'No'\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((adjK-1)**self.drif))),v[0],' '*(self.box_len - len(v[0])),\\\n",
    "                                          ' '*self.right_for_left_node,v[2],' '*(self.box_len - len(v[2])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((adjK-1)**self.drif))),f\"Entropy: {self.print_holder[k+1][3]}\",\\\n",
    "                                          ' '*(self.box_len-len(f\" Entropy: {self.print_holder[k+1][3]}\")) ,' '*self.right_for_left_node, '='*self.box_len)  \n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((adjK-1)**self.drif))),'='*self.box_len,'\\n'*3)  \n",
    "\n",
    "                                    self.current_sate = 'left node'\n",
    "                                    self.left_state += 1\n",
    "                                    break\n",
    "\n",
    "\n",
    "                            # right leaf, left leaf (last node)\n",
    "                            if len(v) == 4:\n",
    "                                if self.current_sate == 'root':\n",
    "\n",
    "                                    print(' '*self.left_root,'='*self.box_len, ' '*self.right_root, '='*self.box_len)\n",
    "                                    print(' '*self.left_root,f'LEFT leaf Level {k}',' '*(self.box_len - len('Left leaf Level k')),\\\n",
    "                                          ' '*self.right_root,f'RIGHT leaf Level {k}',' '*(self.box_len - len('Right leaf Level k')))\n",
    "                                    # Numerical\n",
    "                                    if type(v[1]) == int and type(v[3]) == int: # left and right leafs\n",
    "                                        if v[1] == 1:\n",
    "                                            v[1] = 'Yes'\n",
    "                                            v[3] = 'No'\n",
    "                                        else:\n",
    "                                            v[1] = 'No'\n",
    "                                            v[3] = 'Yes'   \n",
    "\n",
    "                                    print(' '*self.left_root,v[1],' '*(self.box_len - len(v[1])),\\\n",
    "                                          ' '*self.right_root,v[3],' '*(self.box_len - len(v[3])))\n",
    "\n",
    "                                    print(' '*self.left_root,'='*self.box_len, ' '*self.right_root, '='*self.box_len,'\\n'*3) \n",
    "\n",
    "                                    self.current_sate = 'right left leaf'\n",
    "                                    break\n",
    "\n",
    "                                elif self.current_sate == 'right node':\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),'='*self.box_len, ' '*self.right_for_right_node, '='*self.box_len)\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),f'LEFT leaf Level {k}',' '*(self.box_len - len('Left leaf Level k')),\\\n",
    "                                          ' '*self.right_for_left_node,f'RIGHT leaf Level {k}',' '*(self.box_len - len('Right leaf Level k')))\n",
    "                                    # Numerical\n",
    "                                    if type(v[1]) == int and type(v[3]) == int: # left and right leafs\n",
    "                                        if v[1] == 1:\n",
    "                                            v[1] = 'Yes'\n",
    "                                            v[3] = 'No'\n",
    "                                        else:\n",
    "                                            v[1] = 'No'\n",
    "                                            v[3] = 'Yes' \n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),v[1],' '*(self.box_len - len(v[1])),\\\n",
    "                                          ' '*self.right_for_left_node,v[3],' '*(self.box_len - len(v[3])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_right_node+(k-1)**self.drif)),'='*self.box_len, ' '*self.right_for_right_node, '='*self.box_len,'\\n'*3)  \n",
    "\n",
    "                                    self.current_sate = 'right left leaf'\n",
    "                                    break\n",
    "\n",
    "                                elif self.current_sate == 'left node':\n",
    "                                    self.left_state +=1\n",
    "\n",
    "                                    if self.left_state == 2 and k == 3:\n",
    "                                        # Adjust K for left node\n",
    "                                        adjK = k + 1.3\n",
    "                                    else: \n",
    "                                        adjK = 1\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((adjK-1)**self.drif))),'='*self.box_len, ' '*self.right_for_left_node, '='*self.box_len)\n",
    "                                    print(' '*int((self.left_for_left_node-((adjK-1)**self.drif))),f'LEFT leaf Level {k}',' '*(self.box_len - len('Left leaf Level k')),\\\n",
    "                                          ' '*self.right_for_left_node,f'RIGHT leaf Level {k}',' '*(self.box_len - len('Right leaf Level k')))\n",
    "\n",
    "                                    if type(v[1]) == int and type(v[3]) == int: # left and right leafs\n",
    "                                        # Numerical\n",
    "                                        if v[1] == 1:\n",
    "                                            v[1] = 'Yes'\n",
    "                                            v[3] = 'No'\n",
    "                                        else:\n",
    "                                            v[1] = 'No'\n",
    "                                            v[3] = 'Yes' \n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((adjK-1)**self.drif))),v[1],' '*(self.box_len - len(v[1])),\\\n",
    "                                          ' '*self.right_for_left_node,v[3],' '*(self.box_len - len(v[3])))\n",
    "\n",
    "                                    print(' '*int((self.left_for_left_node-((adjK-1)**self.drif))),'='*self.box_len, ' '*self.right_for_left_node, '='*self.box_len,'\\n'*3)   \n",
    "\n",
    "                                    self.current_sate = 'right left leaf'\n",
    "                                    break\n",
    "        else:\n",
    "            pprint(self.print_tree_higher_depth, width=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d5d7a",
   "metadata": {},
   "source": [
    "## Weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddc7558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Seed to replicate results\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf338d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Decision Tree Classifier accuracy is: 100.0 %\n",
      "\n",
      "The time takes to fit the model and calculate its accuracy is 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "# Import dataset\n",
    "data = pd.read_csv('ADD_YOUR_FILE_PATH')\n",
    "\n",
    "# Assign binary values to target Y using variable decision\n",
    "data[\"Y\"] = pd.Categorical(data[\"Decision\"]).codes\n",
    "\n",
    "# Get dataframe \n",
    "df = data[['Outlook','Temperature','Humidity','Wind','Y']]\n",
    "\n",
    "# Instantiate model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Split dataset into test and train sets\n",
    "train_df, test_df = model.train_test_split(df, test_size=0.30)\n",
    "\n",
    "# Fit model \n",
    "tree = model.fit(train_df, max_depth = 3, min_samples_split = 2)\n",
    "\n",
    "# Print model accuracy\n",
    "accuracy = model.accuracy_score(test_df, tree)\n",
    "\n",
    "time_end = time.time() - time_start\n",
    "print(accuracy)\n",
    "print(f'\\nThe time takes to fit the model and calculate its accuracy is {round((time_end),2)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a21ba7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Decision Tree Classifier has a Precision of 100.0% and Recall of 100.0%'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.calculate_precision_recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35a814fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                             ======================\n",
      "                                                                     ROOT      \n",
      "                                                             Outlook = Overcast\n",
      "                                                             Entropy: 0.79\n",
      "                                                             ====================== \n",
      "\n",
      "\n",
      "                                      =========================                 =========================\n",
      "                                      LEFT leaf Level 1                          RIGHT NODE Level 1        \n",
      "                                      Yes                                        Temperature = Hot         \n",
      "                                      =========================                  Entropy: 0.8\n",
      "                                                                                ========================= \n",
      "\n",
      "\n",
      "                                                              =========================                 =========================\n",
      "                                                              LEFT leaf Level 2                          RIGHT NODE Level 2        \n",
      "                                                              No                                         Wind = Weak               \n",
      "                                                              =========================                  Entropy: 0.8\n",
      "                                                                                                        ========================= \n",
      "\n",
      "\n",
      "\n",
      "                                                                             =========================                 =========================\n",
      "                                                                             LEFT leaf Level 3                          RIGHT leaf Level 3        \n",
      "                                                                             Yes                                        No                        \n",
      "                                                                             =========================                 ========================= \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print decision tree\n",
    "dict_tree = model.get_dict_tree(tree)\n",
    "model.tree_print(dict_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cdde0c",
   "metadata": {},
   "source": [
    "## Iris Flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8e5fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Decision Tree Classifier accuracy is: 96.0 %\n",
      "\n",
      "The time takes to fit the model and calculate its accuracy is 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Import dataset\n",
    "df = pd.read_csv(\"ADD_YOUR_FILE_PATH\")\n",
    "\n",
    "# Drop Id column\n",
    "df = df.drop(\"Id\", axis=1)\n",
    "\n",
    "# Rename target as Y \n",
    "iris_df = df.rename(columns={\"species\": \"Y\"})\n",
    "\n",
    "# Instantiate model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Split dataset into test and train sets\n",
    "train_df, test_df = model.train_test_split(iris_df, test_size=0.30)\n",
    "\n",
    "# Fit model \n",
    "tree = model.fit(train_df, max_depth = 3, min_samples_split = 2)\n",
    "\n",
    "# Print model accuracy\n",
    "accuracy = model.accuracy_score(test_df, tree)\n",
    "\n",
    "time_end = time.time() - time_start\n",
    "print(accuracy)\n",
    "print(f'\\nThe time takes to fit the model and calculate its accuracy is {round((time_end),2)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0188629d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                             ======================\n",
      "                                                                     ROOT      \n",
      "                                                             petal_width <= 0.8\n",
      "                                                             Entropy: 1.55\n",
      "                                                             ====================== \n",
      "\n",
      "\n",
      "                                      =========================                 =========================\n",
      "                                      LEFT leaf Level 1                          RIGHT NODE Level 1        \n",
      "                                      Iris-setosa                                petal_width <= 1.65       \n",
      "                                      =========================                  Entropy: 0.97\n",
      "                                                                                ========================= \n",
      "\n",
      "\n",
      "                                                              =========================                 =========================\n",
      "                                                              LEFT NODE Level 2                          RIGHT leaf Level 2        \n",
      "                                                              petal_length <= 4.95                       Iris-virginica            \n",
      "                                                              Entropy: 0.3                              =========================\n",
      "                                                              ========================= \n",
      "\n",
      "\n",
      "\n",
      "                                              =========================                 =========================\n",
      "                                              LEFT leaf Level 3                          RIGHT leaf Level 3        \n",
      "                                              Iris-versicolor                            Iris-virginica            \n",
      "                                              =========================                 ========================= \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_tree = model.get_dict_tree(tree)\n",
    "model.tree_print(dict_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a9b56",
   "metadata": {},
   "source": [
    "## Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12cf9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Decision Tree Classifier accuracy is: 83.0 %\n",
      "\n",
      "The time takes to fit the model and calculate its accuracy is 0.13 seconds\n"
     ]
    }
   ],
   "source": [
    "random.seed(3)\n",
    "time_start = time.time()\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv(\"ADD_YOUR_FILE_PATH\")\n",
    "\n",
    "# Define target Y\n",
    "df[\"Y\"] = df.Survived\n",
    "\n",
    "# Drop unnecessary\n",
    "df = df.drop([\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "median_age = df.Age.median()\n",
    "mode_embarked = df.Embarked.mode()[0]\n",
    "\n",
    "# Fill in missing values\n",
    "titanic_df = df.fillna({\"Age\": median_age, \"Embarked\": mode_embarked})\n",
    "\n",
    "# Instantiate model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Split dataset into test and train sets\n",
    "train_df, test_df = model.train_test_split(titanic_df, test_size=0.20)\n",
    "\n",
    "# Fit model \n",
    "tree = model.fit(train_df, max_depth = 3, min_samples_split = 2)\n",
    "\n",
    "# Print model accuracy\n",
    "accuracy = model.accuracy_score(test_df, tree)\n",
    "\n",
    "time_end = time.time() - time_start\n",
    "print(accuracy)\n",
    "print(f'\\nThe time takes to fit the model and calculate its accuracy is {round((time_end),2)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da3ab523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Decision Tree Classifier has a Precision of 79.41% and Recall of 76.06%'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.calculate_precision_recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "754e9555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                             ==============\n",
      "                                                                 ROOT      \n",
      "                                                             Sex = male\n",
      "                                                             Entropy: 0.93\n",
      "                                                             ============== \n",
      "\n",
      "\n",
      "                                      =========================                 =========================\n",
      "                                      LEFT NODE Level 1                          RIGHT leaf Level 1        \n",
      "                                      Age <= 13.0                                Yes                       \n",
      "                                      Entropy: 0.71                             =========================\n",
      "                                      ========================= \n",
      "\n",
      "\n",
      "\n",
      "                  =========================                 =========================\n",
      "                  LEFT NODE Level 2                          RIGHT leaf Level 2        \n",
      "                  SibSp <= 2.5                               No                        \n",
      "                  Entropy: 0.91                             =========================\n",
      "                  ========================= \n",
      "\n",
      "\n",
      "\n",
      " =========================                 =========================\n",
      " LEFT leaf Level 3                          RIGHT leaf Level 3        \n",
      " Yes                                        No                        \n",
      " =========================                 ========================= \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_tree = model.get_dict_tree(tree)\n",
    "model.tree_print(dict_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce989a2",
   "metadata": {},
   "source": [
    "# Section 2: Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c5d2f",
   "metadata": {},
   "source": [
    "The Decision Tree Regressor has the same underlying architecture of the Classifier with tweaks to make it predict continuous labels using categorical and numerical features. To calculate feature splits, root mean squared error, which is is the standard deviation of the residuals, is used:\n",
    "\n",
    "\\begin{equation*}\n",
    "RMSE = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\dot{x_i})^{2}}{N}}\n",
    "\\end{equation*}\n",
    "\n",
    "Model performance is assessed by method `get_r_squared` which uses the following R squared formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "R^{2} = 1 - \\frac{RSS}{TSS}\n",
    "\\end{equation*}\n",
    "\n",
    "This is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. RSS represents the sum of squared residuals and TSS is the total sum of squares. \n",
    "<br>\n",
    "\n",
    " `grid_search` method can be used to fine tune hyper-parameters `max_depth` and `min_samples_split`. By exhaustively searching the entire parametes space using nested loops and calls to fit the models and get the corresponding $R^2$ value, this method allows to optimise the DecisionTreeRegressor algorithm performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5f9252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "    \n",
    "        def __init_(self):\n",
    "            \n",
    "            self.classify_feature_type = None\n",
    "            self.col_header = None\n",
    "                \n",
    "\n",
    "        def get_purity(self, data):\n",
    "            \n",
    "            \"\"\"\n",
    "            Function to check purity of labels Y.\n",
    "            :param data: dataset.\n",
    "            :return: Boolean true or false.\n",
    "            \"\"\"\n",
    "\n",
    "            label= data[:, -1]\n",
    "            unique_labels = np.unique(label)\n",
    "\n",
    "            if len(unique_labels) == 1:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "\n",
    "        def classify_leaf(self, data):\n",
    "\n",
    "            label = data[:, -1]\n",
    "            leaf = np.mean(label)\n",
    "\n",
    "            return leaf\n",
    "\n",
    "\n",
    "        def features_splits(self, data):\n",
    "            \n",
    "            \"\"\"\n",
    "            :input data: dataset.\n",
    "            :return: most frequent label.\n",
    "            \"\"\"\n",
    "\n",
    "            splits = {}\n",
    "            idx, n_columns = data.shape\n",
    "            for col in range(n_columns - 1):\n",
    "                values = data[:, col]\n",
    "                unique_vals = np.unique(values)\n",
    "                splits[col] = unique_vals\n",
    "\n",
    "            return splits\n",
    "\n",
    "\n",
    "        def data_split(self, data, col_split, val_split):\n",
    "            \n",
    "            \"\"\"\n",
    "            Determine split points for categorical and continuous features.\n",
    "            :input data: dataset.\n",
    "            :return: dictionary of split points by feature.\n",
    "            \"\"\"\n",
    "\n",
    "            split_values = data[:, col_split]\n",
    "            type_of_feature = self.classify_feature_type[col_split]\n",
    "            \n",
    "            if type_of_feature == \"continuous\":\n",
    "                left_df = data[split_values <= val_split]\n",
    "                right_df = data[split_values >  val_split]\n",
    "\n",
    "            # feature is categorical   \n",
    "            else:\n",
    "                left_df = data[split_values == val_split]\n",
    "                right_df = data[split_values != val_split]\n",
    "\n",
    "            return left_df, right_df\n",
    "\n",
    "\n",
    "        def get_mean_squared_error(self, data):\n",
    "            \n",
    "            \"\"\"\n",
    "            Calculate the standard deviation of the residuals\n",
    "            :input data: dataset\n",
    "            :return: root mean squared error\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "            vals = data[:, -1]\n",
    "\n",
    "            if len(vals) == 0:\n",
    "                rmse = 0\n",
    "            else:\n",
    "                preds = np.mean(vals)\n",
    "                rmse = np.mean((vals - preds) **2)\n",
    "\n",
    "            return rmse\n",
    "\n",
    "\n",
    "        def get_metric(self, left_df, right_df, function):\n",
    "            \n",
    "            \"\"\"\n",
    "            Given left and right df calculate RMSE.\n",
    "            :input left_df: left dataset. \n",
    "            :input right_df: 'right dataset.\n",
    "            :return: weighted RMSE\n",
    "            \"\"\"\n",
    "\n",
    "            tot = len(left_df) + len(right_df)\n",
    "            perc_left_df = len(left_df) / tot\n",
    "            perc_right_df = len(right_df) / tot\n",
    "\n",
    "            metric = (perc_left_df * function(left_df) \n",
    "                      + perc_right_df * function(right_df))\n",
    "\n",
    "            return metric\n",
    "\n",
    "\n",
    "        def best_split(self, data, splits):\n",
    "            \n",
    "            \"\"\"\n",
    "            Function to calculate the best feature, its split point and corresponding value. \n",
    "            :input data: dataset. \n",
    "            :input splits: dictionary of features and corresponding split points.\n",
    "            :return: best feature and its value. \n",
    "            \"\"\"\n",
    "\n",
    "            first_cycle = True\n",
    "            for col in splits:\n",
    "                for val in splits[col]:\n",
    "                    left_df, right_df = self.data_split(data, col, val)\n",
    "\n",
    "                    current_metric = self.get_metric(left_df, right_df, function=self.get_mean_squared_error)\n",
    "\n",
    "                    if first_cycle or current_metric <= best_metric:\n",
    "                        first_cycle = False\n",
    "                        best_metric = current_metric\n",
    "                        best_column = col\n",
    "                        best_value = val\n",
    "\n",
    "            return best_column, best_value\n",
    "\n",
    "\n",
    "        def feature_type(self, df):\n",
    "            \n",
    "            \"\"\"\n",
    "            Function to differentiate features based of data type (categorical, continous)\n",
    "            :input df: dataframe.\n",
    "            :return: list of strings for continous and categorical features.\n",
    "            \"\"\"\n",
    "\n",
    "            feature_types = []\n",
    "            values_treshold = 15\n",
    "            for feature in df.columns:\n",
    "                if feature != \"Y\":\n",
    "                    unique_vals = df[feature].unique()\n",
    "                    data = unique_vals[0]\n",
    "\n",
    "                    if (isinstance(data, str)) or (len(unique_vals) <= values_treshold):\n",
    "                        feature_types.append(\"categorical\")\n",
    "                    else:\n",
    "                        feature_types.append(\"continuous\")\n",
    "\n",
    "            return feature_types\n",
    "\n",
    "\n",
    "        def fit(self, df, counter=0, max_depth=3, min_samples_split = 2):\n",
    "        \n",
    "            \"\"\"\n",
    "            Function that builds the decision tree recursively.\n",
    "            :input df: dataframe.\n",
    "            :input max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until \n",
    "                              all leaves contain less than min_samples_split samples.\n",
    "            :input min_samples_split: The minimum number of samples required to split an internal node\n",
    "            :return: whole decision tree structure.\n",
    "            \"\"\"\n",
    "\n",
    "            # data preparations\n",
    "            if counter == 0:\n",
    "                self.col_header = df.columns\n",
    "                self.classify_feature_type = self.feature_type(df)\n",
    "                data = df.values\n",
    "            else:\n",
    "                data = df           \n",
    "\n",
    "\n",
    "            # base cases\n",
    "            if (self.get_purity(data)) or (len(data) < min_samples_split) or (counter == max_depth):\n",
    "                leaf = self.classify_leaf(data)\n",
    "                return leaf\n",
    "\n",
    "\n",
    "            # recursive part\n",
    "            else:    \n",
    "                counter += 1\n",
    "\n",
    "                # helper functions \n",
    "                splits = self.features_splits(data)\n",
    "                col_split, val_split = self.best_split(data, splits)\n",
    "                data_below, data_above = self.data_split(data, col_split, val_split)\n",
    "\n",
    "                # check for empty data\n",
    "                if len(data_below) == 0 or len(data_above) == 0:\n",
    "                    leaf = self.classify_leaf(data)\n",
    "                    return leaf\n",
    "\n",
    "                # determine question\n",
    "                feature_name = self.col_header[col_split]\n",
    "                feature_type = self.classify_feature_type[col_split]\n",
    "                if feature_type == \"continuous\":\n",
    "                    question = \"{} <= {}\".format(feature_name, val_split)\n",
    "\n",
    "                # feature is categorical\n",
    "                else:\n",
    "                    question = \"{} = {}\".format(feature_name, val_split)\n",
    "\n",
    "                # instantiate sub-tree\n",
    "                sub_tree = {question: []}\n",
    "\n",
    "                # find answers (recursion)\n",
    "                yes_answer = self.fit(data_below, counter, max_depth, min_samples_split)\n",
    "                no_answer = self.fit(data_above, counter, max_depth, min_samples_split)\n",
    "\n",
    "                if yes_answer == no_answer:\n",
    "                    sub_tree = yes_answer\n",
    "                else:\n",
    "                    sub_tree[question].append(yes_answer)\n",
    "                    sub_tree[question].append(no_answer)\n",
    "\n",
    "                return sub_tree\n",
    "            \n",
    "            \n",
    "        def predict(self, data, tree):\n",
    "            \n",
    "            \"\"\"\n",
    "            Function to predict new data using the fitted tree.\n",
    "            :input data: data to predict.\n",
    "            :input tree: fitted tree. \n",
    "            :return: predictions\n",
    "            \"\"\"\n",
    "                        \n",
    "            qq = list(tree.keys())[0]\n",
    "            feature_name, operator, value = qq.split(\" \")\n",
    "            \n",
    "            # ask question\n",
    "            if operator == \"<=\":\n",
    "                if data[feature_name] <= float(value):\n",
    "                    answer = tree[qq][0]\n",
    "                else:\n",
    "                    answer = tree[qq][1]\n",
    "\n",
    "            # feature is categorical\n",
    "            else:\n",
    "                if str(data[feature_name]) == value:\n",
    "                    answer = tree[qq][0]\n",
    "                else:\n",
    "                    answer = tree[qq][1]\n",
    "\n",
    "            # base case\n",
    "            if not isinstance(answer, dict):\n",
    "                return answer\n",
    "\n",
    "            # recursive part\n",
    "            else:\n",
    "                residual_tree = answer\n",
    "                return self.predict(data, residual_tree)\n",
    "        \n",
    "        \n",
    "        def get_r_squared(self, df, tree):    \n",
    "            \n",
    "            \"\"\"\n",
    "            Calculate the residual sum of squares.\n",
    "            :input df: dataset.\n",
    "            :input tree: fitted tree.\n",
    "            :return: residual sum of squares. \n",
    "            \"\"\"\n",
    "            \n",
    "            labels = df.Y\n",
    "            mean = labels.mean()\n",
    "            preds = df.apply(self.predict, args=(tree,), axis=1)\n",
    "\n",
    "            ss_res = sum((labels - preds) ** 2)\n",
    "            ss_tot = sum((labels - mean) ** 2)\n",
    "            r_squared = 1 - ss_res / ss_tot\n",
    "\n",
    "            return r_squared\n",
    "        \n",
    "        def train_test_split(self, df, test_size):\n",
    "\n",
    "            \"\"\"\n",
    "            Split data into train and test.\n",
    "            :param df: entire dataset.\n",
    "            :param test_size: if < 0 -> % of dataset allocated to test else number of rows.\n",
    "            return: train and test datasets.\n",
    "            \"\"\"\n",
    "\n",
    "            # Check if test size if float\n",
    "            if isinstance(test_size, float):\n",
    "                test_size = round(test_size * len(df))\n",
    "\n",
    "            idx = df.index.tolist()\n",
    "\n",
    "            # Get test data\n",
    "            test_rows = random.sample(population=idx, k=test_size)\n",
    "\n",
    "            test_df = df.loc[test_rows]\n",
    "            train_df = df.drop(test_rows)\n",
    "\n",
    "            return train_df, test_df\n",
    "\n",
    "        \n",
    "        def grid_search(self, \n",
    "                        train_df, \n",
    "                        test_df,\n",
    "                        max_depth_min=1, \n",
    "                        max_depth_max=7, \n",
    "                        min_samples_split_min=5, \n",
    "                        min_samples_split_max=20,\n",
    "                        steps=5\n",
    "                       ):\n",
    "            \n",
    "            \"\"\"\n",
    "            Iterative function that exhaustively loops through the parameter space testing the performance\n",
    "            of each parameter setting. \n",
    "            :input train_df: train dataset.\n",
    "            :input test_df: test dataset.\n",
    "            :input max_depth_min: max_depth floor value. \n",
    "            :input max_depth_max: max_depth ceiling value.\n",
    "            :input min_samples_split_min: min_samples_split_min floor value.  \n",
    "            :input min_samples_split_max: min_samples_split_min ceiling value. \n",
    "            :input steps: step value used in range loop. \n",
    "            :return: dataframe with train and test R^2 values for each parameter setting.  \n",
    "            \"\"\"\n",
    "\n",
    "            grid_search = {\"max_depth\": [], \n",
    "                           \"min_samples_split\": [], \n",
    "                           \"r_squared_train\": [], \n",
    "                           \"r_squared_test\": []\n",
    "                          }\n",
    "\n",
    "            # Set values to loop through\n",
    "            for max_depth in range(max_depth_min, max_depth_max):\n",
    "                for min_samples_split in range(min_samples_split_min, min_samples_split_max, steps):\n",
    "\n",
    "                    # Fit model \n",
    "                    tree = model.fit(train_df, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "\n",
    "                    # Calculate R squared for train test\n",
    "                    r_squared_train = model.get_r_squared(train_df, tree)\n",
    "                    r_squared_test = model.get_r_squared(test_df, tree)\n",
    "\n",
    "                    # Append values \n",
    "                    grid_search[\"max_depth\"].append(max_depth)\n",
    "                    grid_search[\"min_samples_split\"].append(min_samples_split)\n",
    "                    grid_search[\"r_squared_train\"].append(r_squared_train)\n",
    "                    grid_search[\"r_squared_test\"].append(r_squared_test)\n",
    "\n",
    "                #print(f\"Epoch number:  {max_depth}/{max_depth_max-max_depth_min-1}\")\n",
    "\n",
    "            grid_search = pd.DataFrame(grid_search)\n",
    "            grid_search.sort_values(\"r_squared_test\", ascending=False).head()\n",
    "\n",
    "            return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedc561",
   "metadata": {},
   "source": [
    "##  House Price Predicition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23eb2840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sqft_living <= 2910.0': [{'sqft_living <= 2080.0': [{'bathrooms <= 1.0': [{'yr_built <= 1929.0': [{'sqft_living <= 900.0': [302523.2275565319,\n",
      "                                                                                                                              440168.7106807535]},\n",
      "                                                                                                    {'yr_built <= 1955.0': [321708.5557862696,\n",
      "                                                                                                                            258936.67930629101]}]},\n",
      "                                                                            {'sqft_lot <= 7793.0': [{'sqft_lot <= 7790.0': [439730.21171713696,\n",
      "                                                                                                                            26590000.0]},\n",
      "                                                                                                    {'sqft_living <= 1651.0': [345265.91310736234,\n",
      "                                                                                                                               415316.6359092838]}]}]},\n",
      "                                                      {'yr_built <= 1956.0': [{'yr_built <= 1955.0': [{'floors = 2.0': [858040.7846153846,\n",
      "                                                                                                                        668138.0941370751]},\n",
      "                                                                                                      {'bathrooms <= 2.0': [504290.625,\n",
      "                                                                                                                            3906500.0]}]},\n",
      "                                                                              {'sqft_living <= 2600.0': [{'sqft_living <= 2257.0': [464733.5816741781,\n",
      "                                                                                                                                    534091.5040823584]},\n",
      "                                                                                                         {'bedrooms = 3.0': [700573.5890652592,\n",
      "                                                                                                                             597809.1984137923]}]}]}]},\n",
      "                           {'sqft_living <= 6070.0': [{'sqft_living <= 4160.0': [{'yr_built <= 1946.0': [{'floors = 1.5': [918954.1,\n",
      "                                                                                                                           1285712.367521282]},\n",
      "                                                                                                         {'sqft_living <= 3753.0': [772510.0037836054,\n",
      "                                                                                                                                    964948.0869565217]}]},\n",
      "                                                                                 {'sqft_lot <= 28078.0': [{'sqft_lot <= 28008.0': [1379048.85,\n",
      "                                                                                                                                   3710000.0]},\n",
      "                                                                                                          {'sqft_living <= 4290.0': [817190.625,\n",
      "                                                                                                                                     1079498.3870967743]}]}]},\n",
      "                                                      {'waterfront = 1.0': [5865250.0,\n",
      "                                                                            {'bathrooms <= 3.0': [4489000.0,\n",
      "                                                                                                  {'floors = 2.0': [1709054.2222222222,\n",
      "                                                                                                                    2860000.0]}]}]}]}]}\n",
      "The Decision Tree Classifier accuracy is: 83.0 %\n",
      "\n",
      "The time takes to fit the model is 1.14 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "random.seed(108)\n",
    "\n",
    "# Import dataset\n",
    "df = pd.read_csv(\"ADD_YOUR_FILE_PATH\")\n",
    "\n",
    "# Set Y label\n",
    "df['Y'] = df.price.copy()\n",
    "\n",
    "# Select features\n",
    "df = df[['bedrooms','bathrooms','sqft_living','floors','waterfront','sqft_lot','condition','yr_built','Y']]\n",
    "\n",
    "# Instantitate model\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Split dataset into test and train sets\n",
    "train_df, test_df = model.train_test_split(df, test_size=0.1)\n",
    "\n",
    "# Fit model\n",
    "tree = model.fit(train_df, max_depth = 5, min_samples_split = 10)\n",
    "pprint(tree)\n",
    "\n",
    "time_end = time.time() - time_start\n",
    "print(accuracy)\n",
    "print(f'\\nThe time takes to fit the model is {round((time_end),2)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856a577",
   "metadata": {},
   "source": [
    "This section explains how the above decision tree can be read. \n",
    "<br>\n",
    "`sqft_living <= 2910.0` is the root node. From there it branches off with two nodes, `sqft_living <= 2080.0` if the answer to the node question is positive and `sqft_living <= 6070.0` if negative. \n",
    "<br>\n",
    "Taking the positive node as an example you then encounter two nodes `'bathrooms <= 1.0` if the answer to `sqft_living <= 2080.0`is positive or `yr_built <= 1956.0` if otherwise. \n",
    "<br>\n",
    "Assuming the answer is negative you then have two more nodes: `yr_built <= 1955.0` if the answer to `yr_built <= 1956.0` is positive and `sqft_living <= 2600.0` if the answer is negative. \n",
    "<br>\n",
    "If the house was built before 1955 we then arrive at node `floors = 2.0` which has two leafs. If the house has two floors the predicted price is 858040.78 otherwise 668138.09."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebff2680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>r_squared_train</th>\n",
       "      <th>r_squared_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.765596</td>\n",
       "      <td>0.588987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_samples_split  r_squared_train  r_squared_test\n",
       "15          6                  5         0.765596        0.588987"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(108)\n",
    "r_squared_train = model.grid_search(train_df, test_df)\n",
    "r_squared_train.sort_values(by=['r_squared_train', 'r_squared_test'],ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f58495",
   "metadata": {},
   "source": [
    "# Section 3: Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cbbc31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ed77f",
   "metadata": {},
   "source": [
    "## Sklearn Classifier on Iris Flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8158df63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.3 %\n",
      "\n",
      "The time takes to fit the model and calculate its accuracy is 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Split features and labels \n",
    "X = iris_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width' ]]\n",
    "y = iris_df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=128)\n",
    "\n",
    "# Initialise Decision Tree\n",
    "tree_model = DecisionTreeClassifier() \n",
    " \n",
    "# Fit model\n",
    "tree_model = tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy \n",
    "acc_secore = round(accuracy_score(y_pred, y_test), 3)\n",
    "print('Accuracy: ', round((acc_secore* 100),2),'%')\n",
    "\n",
    "time_end = time.time() - time_start\n",
    "print(f'\\nThe time takes to fit the model and calculate its accuracy is {round((time_end),2)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560795e7",
   "metadata": {},
   "source": [
    "## Section 4: Hypothesis Testing on Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841708a2",
   "metadata": {},
   "source": [
    "In the following section a statistical answer to the question 'Did people in First class have a better chance of survival than people in Third' will be provided. To achive this a Null and Altrnative hypothesis are set as follow: \n",
    "\n",
    "\\begin{equation*}\n",
    "H_o: People \\ in \\ First \\ class \\ had \\ same \\ chance \\ of \\ survival \\ than \\ in \\ Third\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\\begin{equation*}\n",
    "H_1: People \\ in \\ First \\ class \\ had \\ higher \\ chance \\ of \\ survival \\ than \\ in \\ Third\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "Next we look at the data and see what results we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3739e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats as st\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c56c6a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAabUlEQVR4nO3de7wcdZ3m8c9DkJsEIiYoJIEgxMEMXhZixPVCVFhARlDxEtQdcUR0RnRcBxzYUQYRd1FnmeGSUYOjKIgBdGWiROINvKBoTgCJSURjEk2CYgj3O4Fn/qhfsOn0OaeTk+qTnHrer1e/Tl1+XfWt6j79dP2qu1q2iYiI5tpmuAuIiIjhlSCIiGi4BEFERMMlCCIiGi5BEBHRcAmCiIiGSxCMYJLuk/Ss4a5jMJKmS1q1ife9SNJZA8yvfR9IWiHp0DrX0QSSviXp7ZthOddKOmFz1NQU2w53ATF0klYAzwAea5n8bNs7b+LypgOX2J4w5OKGQNJewOKWSU8FHgDWf/nlyMGWsan7YCQpz48TbH93uGsZiO1BH8+oR44IRo7X2N655XbrQI0ljepVYZvK9u9bt6lMfn7LtB8NZflbwz4YjKSt4s3c1lJnUyUIRjBJlrRfGb5I0qclzZV0P/AKSa+WtFjSvZJWSzpZ0lOBbwF7lm6V+yTt2WHZR0m6UdI9klZKOqNl3qSy7rdL+r2k2yX9U8v8HUs9d0paDLxwiJv6NElXle34maR9h7IPBtif75K0pLRdLOnADm2mSfqppLsk/UHSBZK2K/Mk6V8l/anst4WSDijzuqpD0vGSrivLWQucIWlfSd+XtLbs6y9LGlPaXwzsBXyjPJYfKtMPlvSTUucvylFgf9v9j6WmeyXdIulVLfvzrJZ2T+riK11m/yjpZuD+MvzVtmWfK+m8MnytpBMkbV/qOqCl3ThJD0raXdLTJH1T0pryHPqmpGE9et3q2c5tK78BK4BDO0w3sF8Zvgi4G3gJ1RuAHYA/AC8r858GHFiGpwOrBlnndOC5ZVnPA24DXlvmTSrrvhDYEXg+8DDwnDL/bOBHwG7AROCXg62vfXtapl0ErAWmUXV1fhmYPdR90GHdbwRWU4WWgP2Avdv3P3AQcHCpZRKwBPhAmXc4sAAYU5bxHGCPMq/bOo4H1gHvK+vYsdRyGLA9MA74IfBv/T0/gPFln7267IfDyvi4Duv7C2AlsGfLY7tvy/48q+05saptvTeVx3hHYG+qrr3RZf6ost0Hl/FrqbqwAD4PfLxlWe8Fri7DTweOBXYCRgNXAFe2tH1iObl1d8sRwchxZXkXdZekK/tp85+2r7P9uO2HgEeBKZJ2sX2n7Ru6XZnta20vLMu6GfgKcEhbs4/aftD2L4BfUAUCwJuo/snvsL0SOG9jNrSDr9v+ue11VEHwggHabuo+OAH4pO35riy1/bv2RrYX2L7e9jrbK4DP8uf98ijVC9f+gGwvsf2HlnndPha32j6/rOPBUst3bD9sew1wDhs+Fq3eBsy1Pbfsh+8AfVTB0O4xqoCZIukptlfY/u0Ay253nu2Vpc7fATcAryvzXgk8YPv6Dve7FJjRMv6WMg3ba21/zfYDtu8FPj7I9sYgEgQjx2ttjym31/bTZmXb+LFU//y/k/QDSS/udmWSXiTpmnJ4fjfwHmBsW7M/tgw/AKzv59+zrZYNXlA3Un/r6WRT98FEYNAXQEnPLl0Vf5R0D/B/KPvF9veBC4CZwJ8kzZK0y0bWscE2SHqGpNml++Ye4BI2fCxa7Q28seWNw13AS4E92hvaXgp8ADij1DxbHboKu62V6sX8uDL8xIt7B9cAO5Xn2SSqcP86gKSdJH1W0u/K9v4QGKMRcM5nuCQImuVJl5ot726PAXYHrgQu79SuH5cCc4CJtncFPkPV3dGNP1C9sK63V5f32xy63QftVgL79jOv1aeBXwGTbe8C/G9a9ovt82wfBEwBng2cspF1bLANVGFj4LllnW/jyY9Fe/uVwMUtbxzG2H6q7bM7rsy+1PZLqQLEwCfKrPupumfWe2YXtV4BTC99+q+jnyCw/RjVPjiu3L5Z3v0D/ANVl9WLyva+vEzv9vkXbRIEDSVpO0lvlbSr7UeBe4DHy+zbgKdL2nWARYwG7rD9kKRpVO/uunU5cFo56TeBqr+75wbZB+0+B5ws6aBy0nc/SXt3aDe6LOc+SfsDf9uyvheWd7hPoXoRfQh4fCPr6GQ0cB9wt6TxlHBpcRvQ+l2KS4DXSDpc0ihJO5QTvRuccJX0F5JeKWn7Uu+DLbXdBLxa0m6Snkl15DCg0nV1LfAFYLntJQM0vxR4M/BWnhwYo0sdd0naDfjnwdYbA0sQNNv/BFaUw+v3UP3DYftXVH3+y0rXQaeugL8DzpR0L3A6A7+DbfdRqu6g5cC3gYs3fROGrOM+aGf7Cqq+6EuBe6nete/WoenJVKF4L9XJ8sta5u1Spt1Jtf1rgU9tTB39+ChwINWJ8KuA/982//8CHy6P5cnlvMwxVEcra6iOEE6h8+vB9lQn92+n6oLbHTitzLuY6tzPCqrH8bIO9+/kUuBQ+u8WAsD2z6gCc0+qT7Kt929UJ59vB64Hru5yvdEP2flhmoiIJssRQUREwyUIIiIaLkEQEdFwCYKIiIbb6i4ENXbsWE+aNGm4y4iI2KosWLDgdtvjOs3b6oJg0qRJ9PX1DXcZERFbFUn9foM/XUMREQ2XIIiIaLgEQUREwyUIIiIaLkEQEdFwCYKIiIZLEERENFyCICKi4RIEERENt9V9s3goJp161XCXEFuwFWcfNdwlRAyLHBFERDRcgiAiouESBBERDZcgiIhouARBRETD1RoEko6QdIukpZJO7afNmyQtlrRI0qV11hMRERuq7eOjkkYBM4HDgFXAfElzbC9uaTMZOA14ie07Je1eVz0REdFZnUcE04CltpfZfgSYDRzT1uZdwEzbdwLY/lON9URERAd1BsF4YGXL+KoyrdWzgWdLuk7S9ZKO6LQgSSdK6pPUt2bNmprKjYhopuE+WbwtMBmYDhwHXChpTHsj27NsT7U9ddy4jr+9HBERm6jOIFgNTGwZn1CmtVoFzLH9qO3lwK+pgiEiInqkziCYD0yWtI+k7YAZwJy2NldSHQ0gaSxVV9GyGmuKiIg2tQWB7XXAScA8YAlwue1Fks6UdHRpNg9YK2kxcA1wiu21ddUUEREbqvXqo7bnAnPbpp3eMmzgg+UWERHDYLhPFkdExDBLEERENFyCICKi4RIEERENlyCIiGi4BEFERMMlCCIiGi5BEBHRcAmCiIiGSxBERDRcgiAiouESBBERDZcgiIhouARBRETDJQgiIhouQRAR0XAJgoiIhksQREQ0XIIgIqLhEgQREQ2XIIiIaLgEQUREwyUIIiIaLkEQEdFwtQaBpCMk3SJpqaRTO8w/XtIaSTeV2wl11hMRERvatq4FSxoFzAQOA1YB8yXNsb24relltk+qq46IiBhYnUcE04CltpfZfgSYDRxT4/oiImIT1BkE44GVLeOryrR2x0q6WdJXJU3stCBJJ0rqk9S3Zs2aOmqNiGis4T5Z/A1gku3nAd8Bvtipke1Ztqfanjpu3LieFhgRMdLVGQSrgdZ3+BPKtCfYXmv74TL6OeCgGuuJiIgO6gyC+cBkSftI2g6YAcxpbSBpj5bRo4ElNdYTEREd1PapIdvrJJ0EzANGAZ+3vUjSmUCf7TnA+yUdDawD7gCOr6ueiIjorLYgALA9F5jbNu30luHTgNPqrCEiIgY23CeLIyJimCUIIiIaLkEQEdFwCYKIiIZLEERENFyCICKi4RIEERENlyCIiGi4BEFERMMlCCIiGi5BEBHRcAmCiIiGSxBERDRcgiAiouESBBERDZcgiIhouARBRETDJQgiIhouQRAR0XCDBoGknSR9RNKFZXyypL+qv7SIiOiFbo4IvgA8DLy4jK8GzqqtooiI6KlugmBf258EHgWw/QCgWquKiIie6SYIHpG0I2AASftSHSFERMQIsG0Xbc4ArgYmSvoy8BLgHXUWFRERvTPoEYHtbwOvB44HvgJMtX1NNwuXdISkWyQtlXTqAO2OlWRJU7usOyIiNpNuPjX0PdtrbV9l+5u2b5f0vS7uNwqYCRwJTAGOkzSlQ7vRwN8DP9v48iMiYqj6DQJJO0jaDRgr6WmSdiu3ScD4LpY9DVhqe5ntR4DZwDEd2n0M+ATw0MaXHxERQzXQEcG7gQXA/uXv+tt/Ahd0sezxwMqW8VW0BYikA4GJtq8aaEGSTpTUJ6lvzZo1Xaw6IiK61e/JYtvnAudKep/t8zf3iiVtA5xDde5hQLZnAbMApk6d6s1dS0REkw36qSHb50s6gKqff4eW6V8a5K6rgYkt4xPKtPVGAwcA10oCeCYwR9LRtvu6Kz8iIoZq0CCQ9M/AdKogmEt18vfHwGBBMB+YLGkfqgCYAbxl/UzbdwNjW9ZzLXByQiAiore6+ULZG4BXAX+0/Q7g+cCug93J9jrgJGAesAS43PYiSWdKOnoINUdExGbUzRfKHrT9uKR1knYB/sSTu3z6ZXsu1VFE67TT+2k7vZtlRkTE5tVNEPRJGgNcSPWpofuAn9ZZVERE9E43J4v/rgx+RtLVwC62b663rIiI6JUBzxFIGiVpbMukW4GDJS2pt6yIiOiVgb5ZPAO4A7hZ0g8k/Q9gGdWnht7ao/oiIqJmA3UNfRg4yPbS8g3gnwJvsP2N3pQWERG9MFDX0CO2lwLYvgH4TUIgImLkGeiIYHdJH2wZH9M6bvuc+sqKiIheGSgILqS6DER/4xERMQIMdNG5j/aykIiIGB7dXGIiIiJGsARBRETDJQgiIhqu33MEbZ8Y2kA+NRQRMTIM9KmhfEIoIqIB8qmhiIiG6+YXynYA3gn8JU/+qcq/qbGuiIjokW5OFl9M9XvChwM/oPrt4XvrLCoiInqnmyDYz/ZHgPttfxE4CnhRvWVFRESvdBMEj5a/d0k6gOr3inevr6SIiOilbn6qcpakpwEfAeYAO5fhiIgYAboJgi/Yfozq/MCzaq4nIiJ6rJuuoeWSZkl6lSTVXlFERPRUN0GwP/Bd4L3ACkkXSHppvWVFRESvDBoEth+wfbnt1wMvAHah6iaKiIgRoKuLzkk6RNK/AwuovlT2pi7vd4SkWyQtlXRqh/nvkbRQ0k2SfixpykZVHxERQ9bNN4tXADcClwOn2L6/mwVLGgXMBA4DVgHzJc2xvbil2aW2P1PaHw2cAxyxUVsQERFD0s2nhp5n+55NWPY0YKntZQCSZgPHAE8EQdtynwp4E9YTERFDMNBlqD9k+5PAxyVt8AJt+/2DLHs8sLJlfBUdvpEs6b3AB4HtgFf2U8uJwIkAe+211yCrjYiIjTHQEcGS8revzgJszwRmSnoL8GHg7R3azAJmAUydOjVHDRERm9FAl6H+RhlcaPuGTVj2amBiy/iEMq0/s4FPb8J6IiJiCLr51ND/k7RE0sfKtYa6NR+YLGkfSdsBM6guUfEESZNbRo8CfrMRy4+IiM1g0JPFtl8h6ZlUHxn9rKRdgMtsnzXI/dZJOgmYB4wCPm97kaQzgT7bc4CTJB1KdWG7O+nQLRQREfXq5lND2P4jcJ6ka4APAacDAwZBud9cYG7btNNbhv9+o6qNiIjNbtCuIUnPkXSGpIXA+cBPqPr7IyJiBOjmiODzVCdyD7d9a831REREjw0YBOXbwcttn9ujeiIioscG7Boqv0MwsXzqJyIiRqBuuoaWA9dJmgM8cZ0h2+fUVlVERPRMN0Hw23LbBhhdbzkREdFr3XyP4KO9KCQiIoZHN5ehvoYOVwW13fECcRERsXXppmvo5JbhHYBjgXX1lBMREb3WTdfQgrZJ10n6eU31REREj3XTNbRby+g2wEHArrVVFBERPdVN19ACqnMEouoSWg68s86iIiKid7rpGtqnF4VERMTwGOinKl8IrCxXHkXSX1OdKP4dcIbtO3pTYkRzTDr1quEuIbZgK84+qpblDnSJic8CjwBIejlwNvAl4G7Kz0ZGRMTWb6CuoVEt7/rfDMyy/TXga5Juqr2yiIjoiYGOCEZJWh8UrwK+3zKvqx+0iYiILd9AL+hfAX4g6XbgQeBHAJL2o+oeioiIEaDfILD9cUnfA/YAvm17/WUmtgHe14viIiKifgN28di+vsO0X9dXTkRE9Nqgv1kcEREjW4IgIqLhEgQREQ2XIIiIaLhag0DSEZJukbRU0qkd5n9Q0mJJN0v6nqS966wnIiI2VFsQSBoFzASOBKYAx0ma0tbsRmCq7ecBXwU+WVc9ERHRWZ1HBNOApbaX2X4EmA0c09rA9jW2Hyij1wMTaqwnIiI6qDMIxgMrW8ZXlWn9eSfwrU4zJJ0oqU9S35o1azZjiRERsUWcLJb0NmAq8KlO823Psj3V9tRx48b1triIiBGuzovHrQYmtoxPKNOeRNKhwD8Bh9h+uMZ6IiKigzqPCOYDkyXtI2k7YAYwp7WBpP9G9bsHR9v+U421REREP2oLAtvrgJOAecAS4HLbiySdKeno0uxTwM7AFZJukjSnn8VFRERNav1dAdtzgblt005vGT60zvVHRMTgtoiTxRERMXwSBBERDZcgiIhouARBRETDJQgiIhouQRAR0XAJgoiIhksQREQ0XIIgIqLhEgQREQ2XIIiIaLgEQUREwyUIIiIaLkEQEdFwCYKIiIZLEERENFyCICKi4RIEERENlyCIiGi4BEFERMMlCCIiGi5BEBHRcAmCiIiGSxBERDRcrUEg6QhJt0haKunUDvNfLukGSeskvaHOWiIiorPagkDSKGAmcCQwBThO0pS2Zr8HjgcurauOiIgY2LY1LnsasNT2MgBJs4FjgMXrG9heUeY9XmMdERExgDq7hsYDK1vGV5VpG03SiZL6JPWtWbNmsxQXERGVreJkse1Ztqfanjpu3LjhLiciYkSpMwhWAxNbxieUaRERsQWpMwjmA5Ml7SNpO2AGMKfG9UVExCaoLQhsrwNOAuYBS4DLbS+SdKakowEkvVDSKuCNwGclLaqrnoiI6KzOTw1hey4wt23a6S3D86m6jCIiYphsFSeLIyKiPgmCiIiGSxBERDRcgiAiouESBBERDZcgiIhouARBRETDJQgiIhouQRAR0XAJgoiIhksQREQ0XIIgIqLhEgQREQ2XIIiIaLgEQUREwyUIIiIaLkEQEdFwCYKIiIZLEERENFyCICKi4RIEERENlyCIiGi4BEFERMMlCCIiGq7WIJB0hKRbJC2VdGqH+dtLuqzM/5mkSXXWExERG6otCCSNAmYCRwJTgOMkTWlr9k7gTtv7Af8KfKKueiIiorM6jwimAUttL7P9CDAbOKatzTHAF8vwV4FXSVKNNUVERJtta1z2eGBly/gq4EX9tbG9TtLdwNOB21sbSToROLGM3ifplloqbp6xtO3rJlOOR7dEeY62GOJzdO/+ZtQZBJuN7VnArOGuY6SR1Gd76nDXEdGfPEd7o86uodXAxJbxCWVaxzaStgV2BdbWWFNERLSpMwjmA5Ml7SNpO2AGMKetzRzg7WX4DcD3bbvGmiIiok1tXUOlz/8kYB4wCvi87UWSzgT6bM8B/gO4WNJS4A6qsIjeSXdbbOnyHO0B5Q14RESz5ZvFERENlyCIiGi4BMEWStJjkm5quU2S9JONXMYHJO00wPxpkn5YLgNyo6TPSdpJ0vGSLhj6VsRII+npLc/JP0paXYbvkrS4n/ucKenQLpY9XdI3+5n3FElnS/qNpBsk/VTSkWXeCkljh7ZlzbZVfI+goR60/YK2af+9vZGkbW2v62cZHwAuAR7ocL9nAFcAM2z/tEx7AzB6CDXHCGd7LfACAElnAPfZ/pdynbCOL+K2T+80XdIo2491ueqPAXsAB9h+uDx/D9m46qM/OSLYiki6r/ydLulHkuYAiyU9VdJVkn4h6ZeS3izp/cCewDWSrumwuPcCX1wfAgC2v2r7trZ1vqZcEPBGSd8t/4BIOqTlneGNkkZL2qMcYdxU6nhZbTsjtkSjJF0oaZGkb0vaEUDSReVNxvp375+QdAPwxnJhyl+V8dd3Wmg5qn0X8D7bDwPYvs325R3aXilpQanhxDJtVKnhl5IWSvpfZfr7JS2WdLOk2XXskK1Fjgi2XDtKuqkML7f9urb5B1K9O1ou6VjgVttHAUja1fbdkj4IvMJ2p6/oH8Cfr/M0kB8DB9u2pBOADwH/AJwMvNf2dZJ2Bh6iugzIPNsfLxcd7LdbKkakycBxtt8l6XLgWKoj0nZrbR8oaQfgN8ArgaXAZf0sdz/g97bv6aKGv7F9Rwmh+ZK+BkwCxts+AEDSmNL2VGCfcoQxptPCmiJHBFuuB22/oNzaQwDg57aXl+GFwGHlndbLbN+9GeuYAMyTtBA4BfjLMv064Jxy5DGmdE/NB95Rugyea/vezVhHbPmW276pDC+gegHuZP0L/v7lPr8pXyTtFBob6/2SfgFcT3XVgsnAMuBZks6XdASwPlBuBr4s6W1Af92rjZAg2Hrdv37A9q+pjhAWAmdJ2qBPVtLrWrpypgKLgIO6WM/5wAW2nwu8G9ihrPNs4ARgR+A6Sfvb/iHwcqpLh1wk6a+HtIWxtXm4Zfgx+u9xuL+f6U+QNK88Vz9HdbSwl6RdBrnPdOBQ4MW2nw/cCOxg+07g+cC1wHuAz5W7HEV1qfwDqY4eGttD0tgNH0kk7QncYfsSSXdRvUAD3Et18vd2218Hvt5yn5XAzyVdZftnZdrrqd7pt9qVP18j6u0t99/X9kJgoaQXAvtLehBYZftCSdtT/YN9aTNvbowcvwImlefSb4Hj1s+wfXhrQ0n/AZwr6d22H5E0Dphu+4qWZrtS/b7JA5L2Bw4u9x0LPGL7a6quXHyJpG2AibavkfRjqqsa7AzcVd/mbrkSBCPDc4FPSXoceBT42zJ9FnC1pFttv6L1DrZvkzQD+BdJuwOPAz8Erm5b9hnAFZLuBL4P7FOmf0DSK8r9FgHfovpnOkXSo8B9QI4Iol+2HyondK+S9ADwI/r/1NqHgbOoPhzxENVRRfuR79XAeyQtAW6h6h6C6nL3Xygv/gCnUV325hJJuwICzrN91+bZsq1PLjEREdFwOUcQEdFwCYKIiIZLEERENFyCICKi4RIEERENlyCIiGi4BEFERMP9F/aeGRzFsqqgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [\"First-Class\", \"Third-Class\"]\n",
    "\n",
    "# Mean values of First and Third class \n",
    "y = [np.mean(titanic_df[\"Y\"][titanic_df[\"Pclass\"]==1]), np.mean(titanic_df[\"Y\"][titanic_df[\"Pclass\"]==3])]\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.ylabel(\"Survival Rate\")\n",
    "plt.title(\"First and Thirs class rate survival\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c7497",
   "metadata": {},
   "source": [
    "Looking at the plot above, it’s quite clear that First class people had a higher rate of survival. However, this conclusion could be classified as invalid this data is just a sample and dont know if datapoints are random, unbiased and representative of the entire population. \n",
    "<br>\n",
    "<br>\n",
    "This is where Hypothesis testing come into play. Hypothesis testing is used to check if the observed difference between the two populations is significant or is just due to some randomness in the data.\n",
    "<br><br>\n",
    "For this exercise a Z-Score test will be used to conduct the significance test and see if the null hypothesis should be rejected or accepted.\n",
    "<br><br>\n",
    "For the z-test to work, the probability distribution of the survival rate of the first-class people and the probability distribution of the survival rate for the third-class both have to be normally distributed. Let's check if this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f7b4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = np.array([np.mean(titanic_df[titanic_df[\"Pclass\"]==1].sample(20)[\"Y\"].values) for i in range(100)])\n",
    "third_sample = np.array([np.mean(titanic_df[titanic_df[\"Pclass\"]==3].sample(20)[\"Y\"].values) for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120260a5",
   "metadata": {},
   "source": [
    "According to the central limit theorem, our two sample populations should be approximately normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2d756ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/test/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/test/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(1, 2, figsize = (10, 5))\n",
    "plt.subplot(1,2, 1)\n",
    "sns.distplot(first_sample)\n",
    "plt.title(\"First Class Distribution\")\n",
    "plt.xlabel(\"Survival Rate\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(third_sample)\n",
    "plt.title(\"Third-Class Distribution\")\n",
    "plt.xlabel(\"Survival Rate\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a11b24",
   "metadata": {},
   "source": [
    "According to the sample distributions, the effect of the First to Third class is 0.6296–0.2423 = 0.3873\n",
    "<br><br>\n",
    "The z-test checks the significance of this effect by calculating the z-score which is measurement of how many standard deviations a value is from the mean. Tthe higher the z-score, the further the value from the mean and therefore on the extreme sides of the distribution curve.\n",
    "<br>\n",
    "The Z-Score is then turned into the P-value which is the probability of observing that effect given that the null hypothesis is true. In other words, if the P-value is high then we can accept the Null hypothesis since the probability of obtaining that extreme difference in the survival rate (even though when the class doesn’t have an effect) is high.\n",
    "<br>\n",
    "A value of Alpha also needs to be set as a threshold for rejecting the Null hypothesis. For this experiment a level of 0.05 will be set. If the P-value is lower than that level, then the null hypothesis can be rejected.\n",
    "<br>\n",
    "Next the Z-Score can be calculate using the following formula: \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "Z = \\frac{\\mu _{1}-\\mu_{3}}{\\sigma_{1-3}}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "where\n",
    "\n",
    "μ₁ is the mean of the First class\n",
    "<br>\n",
    "μ₃ is the mean of the Third class \n",
    "<br>\n",
    "σ₁₋₃ is the standard deviation of the differences between the two populations\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Worth noting that these parameters don't represent the entire population. Since the dampleset is sufficently large then the standard deviations of each of these samples can be considered as a good approximation to the true population standard deviation using the following formula:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma_{1-3} = \\sqrt{\\frac{\\sigma_{1}^{2}}{n}+\\frac{\\sigma_{3}^{2}}{m}}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "where\n",
    "\n",
    "σ₁ is the standard deviation of the First class sample distribution\n",
    "n is the First class sample distribution size\n",
    "σ₃ is the standard deviation of the Third class sample distribution\n",
    "m is the Third class sample distribution size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a0731c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.62"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean difference\n",
    "effect = np.mean(first_sample) - np.mean(third_sample)\n",
    "\n",
    "# Get First class sigma\n",
    "first_sigma = np.std(first_sample)\n",
    "\n",
    "# Get Third class sigma\n",
    "third_sigma = np.std(third_sample)\n",
    "\n",
    "# Delta sigma \n",
    "sigma_difference = np.sqrt((first_sigma**2)/len(first_sample)  +  (third_sigma**2)/len(third_sample))\n",
    "z_score = round((effect / sigma_difference),2)\n",
    "z_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbf6e2",
   "metadata": {},
   "source": [
    "The z-score above is 26.62 which is an very high score. THe cell below calculates the P-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e20b3dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.983712211029806e-156"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get P-value\n",
    "st.norm.sf(abs(z_score))*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f24b1e",
   "metadata": {},
   "source": [
    "The P-value is 3.98e-156 which is considerably smaller than the level of 0.05 that we set. What this P-value indicates is if we assume that the null hypothesis is true, then the probability of observing that effect by random is 3.98e-156% which is an extremely small probability that makes us comfortable in rejecting the Null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e64ab",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<br>\n",
    "\n",
    "• Iris Flower dataset: https://www.kaggle.com/datasets/arshid/iris-flower-dataset\n",
    "<br>\n",
    "• Titanic dataset: https://www.kaggle.com/datasets/yasserh/titanic-dataset\n",
    "<br>\n",
    "• House Price Prediction dataset: https://www.kaggle.com/datasets/shree1992/housedata\n",
    "<br>\n",
    "• Sklearn - Decision Tree Classifier: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "<br>\n",
    "• Sklearn - Decision Tree Regressor: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a3980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
